import io
import time
import base64

from PIL import Image
from groq import Groq, APIStatusError, InternalServerError

from .base import LLMBase


def encode_image(image: Image.Image, format="jpeg") -> str:
    """
    Convert a PIL Image to a base64 encoded string.

    Args:
        image (Image.Image): The image to convert.

    Returns:
        str: The base64 encoded image string.
    """
    with io.BytesIO() as output:
        image.save(output, format=format)
        return f"data:image/{format};base64,{base64.b64encode(output.getvalue()).decode('utf-8')}"

def wrap_prompt(prompt: list) -> list:
    """
    Wraps the prompt to ensure it is in the correct format for OpenAI.

    Args:
        prompt (list): A list containing the prompt details.

    Returns:
        list: The wrapped prompt.
    """
    return [
        {"role": "user", "content": prompt},
    ]

def wrap_image(image: Image.Image) -> dict:
    """
    Wraps an image to ensure it is in the correct format for OpenAI.

    Args:
        image (Image.Image): The image to wrap.

    Returns:
        dict: The wrapped image.
    """
    return {
        "type": "image_url",
        "image_url": {"url": encode_image(image)},
    }

def wrap_text(text: str) -> dict:
    """
    Wraps text to ensure it is in the correct format for OpenAI.

    Args:
        text (str): The text to wrap.

    Returns:
        dict: The wrapped text.
    """
    return {
        "type": "text",
        "text": text,
    }


PREFIXES = {
    "llama-4-scout-17b-16e-instruct": "meta-llama/",
    "llama-4-maverick-17b-128e-instruct": "meta-llama/",
}


class GroqModel(LLMBase):
    """
    Gemini class for interacting with the Gemini LLM.
    This class extends the LLMBase class and implements the prompt method.
    """
    def __init__(self, model_name, *args, **kwargs):
        if model_name in PREFIXES:
            model_name = PREFIXES[model_name] + model_name
        super().__init__(model_name, *args, **kwargs)

        self.client = Groq()

    def prompt(self, prompt: list) -> str:
        """
        Generate a response from the Gemini model based on the provided prompt.

        Args:
            prompt (list): A list containing the prompt details.

        Returns:
            str: The response generated by the Gemini model.
        """
        prompt = wrap_prompt([
            wrap_image(d) if isinstance(d, Image.Image) else wrap_text(d)
            for d in prompt
        ])

        #print(prompt)
        n_retries = self.retries_on_error

        while True:
            try:
                response = self.client.chat.completions.create(
                    model=self.model_name,
                    messages=prompt,
                )
                break  # Exit loop if successful
            except APIStatusError as e:
                if self.rate_limit_wait and e.status_code == 429:
                    print("Rate limit exceeded. Waiting for 1 minute before retrying...")
                    time.sleep(60)
                    continue
                else:
                    raise e
            except InternalServerError as e:
                if n_retries > 0:
                    print(f"Internal server error: {e}. Retrying in 30 seconds...")
                    time.sleep(30)
                    n_retries -= 1
                    continue
                else:
                    raise e

        return response.choices[0].message.content